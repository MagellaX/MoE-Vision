{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc6d7bf7-4533-4413-af2d-908d30c38cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 12:35:55 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.216.03             Driver Version: 535.216.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   24C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "469aad67-4940-4edb-8b05-f3c80109fa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4436ca59-3ebf-4f7f-bc30-0e14c9dced78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Ensure every computation happens on the GPU when available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba81b4a-1ccf-4acc-a2c4-3b9f535eb4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To build the encoding and decoding functions we use the tinyshakespear dataset. However for the sake of brevity we do not pretrain the decoder model on it\n",
    "#the training function should be able to do it without an issue as well as it could take both images and tex\n",
    "text_path = \"./input.txt\"\n",
    "with open(text_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "stoi['']= 65\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "itos[65] = ''\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "vocab_size = len(stoi.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51c5cefc-fa16-4462-becc-d84a88f9f099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(nn.Module):\n",
    "    def __init__(self, img_size=96, patch_size=16, hidden_dim=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Store the input image size\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # Store the size of each patch\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        # Calculate the total number of patches\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Create a convolutional layer to extract patch embeddings\n",
    "        # in_channels=3 assumes the input image has 3 color channels (RGB)\n",
    "        # out_channels=hidden_dim sets the number of output channels to match the hidden dimension\n",
    "        # kernel_size=patch_size and stride=patch_size ensure each patch is separately embedded\n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=hidden_dim,\n",
    "                              kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Extract patch embeddings from the input image\n",
    "        X = self.conv(X)\n",
    "\n",
    "        # Flatten the spatial dimensions (height and width) of the patch embeddings\n",
    "        # This step flattens the patch dimensions into a single dimension\n",
    "        X = X.flatten(2)\n",
    "\n",
    "        # Transpose the dimensions to obtain the shape [batch_size, num_patches, hidden_dim]\n",
    "        # This step brings the num_patches dimension to the second position\n",
    "        X = X.transpose(1, 2)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d83f9cb7-ad72-499a-b963-d926f2d1860e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 36, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing\n",
    "img_size, patch_size,  num_hiddens, batch_size = 96, 16, 512, 4\n",
    "patch_embeddings = PatchEmbeddings(img_size, patch_size, num_hiddens )\n",
    "X = torch.zeros(batch_size, 3, img_size, img_size)\n",
    "patch_embeddings(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c0431fb-8914-4b29-bb18-84ad348fb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#swapping linear for lazy linear for simplicity. Lazylinear can accept any arbitrary input dimension without having it specified\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_embd, dropout=0.1, is_decoder=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the layers of the MLP\n",
    "        layers = [\n",
    "            # First linear layer that expands the input dimension from n_embd to 4 * n_embd\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "\n",
    "            # Activation function: ReLU if is_decoder is True, else GELU\n",
    "            nn.ReLU() if is_decoder else nn.GELU(),\n",
    "\n",
    "            # Second linear layer that projects the intermediate dimension back to n_embd\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "\n",
    "            # Dropout layer for regularization\n",
    "            nn.Dropout(dropout)\n",
    "        ]\n",
    "\n",
    "        # Create a sequential container to hold the layers\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the MLP layers\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4201b054-ae63-406a-81ec-ccb2f506890c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 128])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For the sake of this example consider embedding size to be 128\n",
    "n_embd = 128\n",
    "testmlp = MLP(n_embd)\n",
    "mlp_input = torch.zeros(batch_size, 3, n_embd)\n",
    "testmlp_out = testmlp(mlp_input)\n",
    "testmlp_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c20e4a1-e772-41b6-a022-402f097dfe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, n_embd, head_size, dropout=0.1, is_decoder=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Linear layer for key projection\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # Linear layer for query projection\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # Linear layer for value projection\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Flag indicating whether this head is used in the decoder\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the batch size (B), sequence length (T), and embedding dimension (C) from the input tensor\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Compute key, query, and value projections\n",
    "        k = self.key(x)   # Shape: [B, T, head_size]\n",
    "        q = self.query(x) # Shape: [B, T, head_size]\n",
    "        v = self.value(x) # Shape: [B, T, head_size]\n",
    "\n",
    "        # Compute attention scores by taking the dot product of query and key\n",
    "        # and scaling by the square root of the embedding dimension\n",
    "        wei = q @ k.transpose(-2, -1) * (C ** -0.5) # Shape: [B, T, T]\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # If this head is used in the decoder, apply a causal mask to the attention scores\n",
    "            # to prevent attending to future positions\n",
    "            tril = torch.tril(torch.ones(T, T, dtype=torch.bool, device=x.device))\n",
    "            wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "\n",
    "        # Apply softmax to the attention scores to obtain attention probabilities\n",
    "        wei = F.softmax(wei, dim=-1) # Shape: [B, T, T]\n",
    "\n",
    "        # Apply dropout to the attention probabilities for regularization\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Perform weighted aggregation of values using the attention probabilities\n",
    "        out = wei @ v # Shape: [B, T, head_size]\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "323aacc0-f24f-4df6-8222-38b9de6f27e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 16])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example values for testing\n",
    "n_embd, head_size, batch_size = 128, 16, 4\n",
    "\n",
    "testhead = Head(n_embd, head_size)\n",
    "head_input = torch.zeros(batch_size, 3, n_embd)\n",
    "testhead_out = testhead(head_input)\n",
    "testhead_out.shape # (B, T,H_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b03c82d1-1c16-4b90-ae02-75fd0c73b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embd, num_heads, dropout=0.1, is_decoder=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Ensure that the embedding dimension is divisible by the number of heads\n",
    "        assert n_embd % num_heads == 0, \"n_embd must be divisible by num_heads\"\n",
    "\n",
    "        # Create a ModuleList of attention heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(n_embd, n_embd // num_heads, dropout, is_decoder)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        # Linear layer for projecting the concatenated head outputs\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "\n",
    "        # Dropout layer for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply each attention head to the input tensor\n",
    "        head_outputs = [h(x) for h in self.heads]\n",
    "\n",
    "        # Concatenate the outputs from all heads along the last dimension\n",
    "        out = torch.cat(head_outputs, dim=-1)\n",
    "\n",
    "        # Apply the projection layer to the concatenated outputs\n",
    "        out = self.proj(out)\n",
    "\n",
    "        # Apply dropout to the projected outputs for regularization\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3395b9d-4b62-417f-9e23-82fa1c14287e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example values for testing\n",
    "n_embd, n_head = 128, 8\n",
    "testmha = MultiHeadAttention(n_embd, n_head)\n",
    "head_input = torch.zeros(batch_size, 3, n_embd)\n",
    "testmha_out = testmha(head_input)\n",
    "testmha_out.shape # (B, T,H_size*n_heads = n_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0af2866-bbb0-4703-8618-f888d3c97502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, num_heads, dropout=0.1, is_decoder=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer normalization for the input to the attention layer\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Multi-head attention module\n",
    "        self.attn = MultiHeadAttention(n_embd, num_heads, dropout, is_decoder)\n",
    "\n",
    "        # Layer normalization for the input to the FFN\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Feed-forward neural network (FFN)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),  # Expand the dimension\n",
    "            nn.GELU(),  # Activation function\n",
    "            nn.Linear(4 * n_embd, n_embd),  # Project back to the original dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        original_x = x  # Save the input for the residual connection\n",
    "\n",
    "        # Apply layer normalization to the input\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        attn_output = self.attn(x)\n",
    "\n",
    "        # Add the residual connection (original input) to the attention output\n",
    "        x = original_x + attn_output\n",
    "\n",
    "        # Apply layer normalization to the input to the FFN\n",
    "        x = self.ln2(x)\n",
    "\n",
    "        # Apply the FFN\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # Add the residual connection (input to FFN) to the FFN output\n",
    "        x = x + ffn_output\n",
    "\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5ada25e-04db-4081-a230-06c7a456eddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example values for testing\n",
    "n_embd, head_size, batch_size = 128, 16, 4\n",
    "\n",
    "testblock = Block(n_embd, n_head)\n",
    "block_input = torch.zeros(batch_size, 3, n_embd)\n",
    "testblock_out = testblock(block_input)\n",
    "testblock_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b23e577e-20e5-4faf-ae3a-a0c8270c061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, num_hiddens, num_heads, num_blks, emb_dropout, blk_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Patch embedding layer to convert the input image into patches\n",
    "        self.patch_embedding = PatchEmbeddings(img_size, patch_size, num_hiddens)\n",
    "\n",
    "        # Learnable classification token\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, num_hiddens))\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "\n",
    "        # Learnable position embedding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, num_hiddens))\n",
    "\n",
    "        # Dropout layer for the embeddings\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([Block(num_hiddens, num_heads, blk_dropout, is_decoder=False) for _ in range(num_blks)])\n",
    "\n",
    "        # Layer normalization for the final representation\n",
    "        self.layer_norm = nn.LayerNorm(num_hiddens)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        # Convert the input image into patch embeddings\n",
    "        x = self.patch_embedding(X)\n",
    "\n",
    "        # Expand the classification token to match the batch size\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "\n",
    "        # Concatenate the classification token with the patch embeddings\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Add the position embedding to the patch embeddings\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        # Apply dropout to the embeddings\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Pass the embeddings through the transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Apply layer normalization to the final representation\n",
    "        x = self.layer_norm(x[:, 0])\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c512072d-17a1-4d2c-ad7e-144222af0047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For purposes of testing\n",
    "img_size, patch_size, num_hiddens, n_head, num_blks, dropout = 96, 16, 512, 8, 3, 0.1\n",
    "\n",
    "testvit = ViT(img_size, patch_size, num_hiddens, n_head, num_blks, dropout, dropout)\n",
    "vit_input = torch.zeros(batch_size, 3, img_size, img_size)\n",
    "testvit_out = testvit(vit_input)\n",
    "testvit_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "054f7f07-fb5a-4192-9cf4-7dbbf870be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalProjector(nn.Module):\n",
    "    def __init__(self, n_embd, image_embed_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define the projection network\n",
    "        self.net = nn.Sequential(\n",
    "            # Linear layer to expand the image embedding dimension\n",
    "            nn.Linear(image_embed_dim, 4 * image_embed_dim),\n",
    "\n",
    "            # GELU activation function\n",
    "            nn.GELU(),\n",
    "\n",
    "            # Linear layer to project the expanded image embeddings to the text embedding dimension\n",
    "            nn.Linear(4 * image_embed_dim, n_embd),\n",
    "\n",
    "            # Dropout layer for regularization\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input through the projection network\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eb15a7f-deb6-4501-9331-9dd415a6728f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example values for testing\n",
    "n_embd,num_hiddens = 128, 512\n",
    "\n",
    "testmmp = MultiModalProjector(n_embd,num_hiddens)\n",
    "mmp_input = testvit_out\n",
    "testmmp_out = testmmp(mmp_input)\n",
    "testmmp_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "508c47c3-e2fe-465e-b372-d3fb9709da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLanguageModel(nn.Module):\n",
    "    def __init__(self, n_embd, image_embed_dim, vocab_size, num_heads, n_layer, use_images=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_images = use_images\n",
    "\n",
    "        # Token embedding table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "\n",
    "        # Position embedding table\n",
    "        self.position_embedding_table = nn.Embedding(1000, n_embd)\n",
    "\n",
    "        if use_images:\n",
    "            # Image projection layer to align image embeddings with text embeddings\n",
    "            self.image_projection = MultiModalProjector(n_embd, image_embed_dim)\n",
    "\n",
    "        # Stack of transformer decoder blocks\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, num_heads, is_decoder=True) for _ in range(n_layer)])\n",
    "\n",
    "        # Final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Language modeling head\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, image_embeds=None, targets=None):\n",
    "        # Get token embeddings from the input indices\n",
    "        tok_emb = self.token_embedding_table(idx)\n",
    "\n",
    "        if self.use_images and image_embeds is not None:\n",
    "            # Project and concatenate image embeddings with token embeddings\n",
    "            img_emb = self.image_projection(image_embeds).unsqueeze(1)\n",
    "            tok_emb = torch.cat([img_emb, tok_emb], dim=1)\n",
    "\n",
    "        # Get position embeddings\n",
    "        pos_emb = self.position_embedding_table(torch.arange(tok_emb.size(1), device=device)).unsqueeze(0)\n",
    "\n",
    "        # Add position embeddings to token embeddings\n",
    "        x = tok_emb + pos_emb\n",
    "\n",
    "        # Pass through the transformer decoder blocks\n",
    "        x = self.blocks(x)\n",
    "\n",
    "        # Apply final layer normalization\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Get the logits from the language modeling head\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            if self.use_images and image_embeds is not None:\n",
    "                # Prepare targets by concatenating a dummy target for the image embedding\n",
    "                batch_size = idx.size(0)\n",
    "                targets = torch.cat([torch.full((batch_size, 1), -100, dtype=torch.long, device=device), targets], dim=1)\n",
    "\n",
    "            # Compute the cross-entropy loss\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-100)\n",
    "            return logits, loss\n",
    "\n",
    "        return logits\n",
    "        \n",
    "    def generate(self, idx, image_embeds, max_new_tokens):\n",
    "        # Get the batch size and sequence length\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Initialize the generated sequence with the input indices\n",
    "        generated = idx\n",
    "\n",
    "        if self.use_images and image_embeds is not None:\n",
    "            # Project and concatenate image embeddings with token embeddings\n",
    "            img_emb = self.image_projection(image_embeds).unsqueeze(1)\n",
    "            current_output = torch.cat([img_emb, self.token_embedding_table(idx)], dim=1)\n",
    "        else:\n",
    "            current_output = self.token_embedding_table(idx)\n",
    "            \n",
    "            # Generate new tokens iteratively\n",
    "        for i in range(max_new_tokens):\n",
    "            # Get the current sequence length\n",
    "            T_current = current_output.size(1)\n",
    "\n",
    "            # Get position embeddings for the current sequence length\n",
    "            current_pos_emb = self.position_embedding_table(torch.arange(T_current, device=device)).unsqueeze(0)\n",
    "\n",
    "            # Add position embeddings to the current output\n",
    "            current_output += current_pos_emb\n",
    "\n",
    "            # Pass through the transformer decoder blocks\n",
    "            for block in self.blocks:\n",
    "                current_output = block(current_output)\n",
    "\n",
    "            # Get the logits for the last token\n",
    "            logits = self.lm_head(current_output[:, -1, :])\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample the next token based on the probabilities\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Concatenate the generated token to the generated sequence\n",
    "            generated = torch.cat((generated, idx_next), dim=1)\n",
    "\n",
    "            # Get the embeddings for the generated token\n",
    "            idx_next_emb = self.token_embedding_table(idx_next)\n",
    "\n",
    "            # Concatenate the generated token embeddings to the current output\n",
    "            current_output = torch.cat((current_output, idx_next_emb), dim=1)\n",
    "\n",
    "        return generated\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d83a3a8b-640d-4bc1-a69a-b02060ad5c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([10, 51, 1000]), Loss: 7.1114301681518555\n",
      "Generated sequence shape: torch.Size([10, 70])\n"
     ]
    }
   ],
   "source": [
    "# I use n_layer to represent number of decoder transformer blocks and n_blks for the vision encoder to avoid confusion\n",
    "model = DecoderLanguageModel(n_embd=128, image_embed_dim=256, vocab_size=1000, num_heads=8, n_layer=6, use_images=True)\n",
    "model.to(device)\n",
    "# Dummy input\n",
    "B, T = 10, 50\n",
    "idx = torch.randint(0, 1000, (B, T)).to(device)\n",
    "image_embeds = torch.randn(B, 256).to(device)  # Assume image_embed_dim is 256\n",
    "\n",
    "targets = torch.randint(0, vocab_size, (B, T)).to(device)  # Only if you want to compute loss\n",
    "\n",
    "# Test forward pass\n",
    "# Check if you need to calculate loss by providing targets\n",
    "if targets is not None:\n",
    "    logits, loss = model(idx, image_embeds, targets)\n",
    "    print(f\"Logits shape: {logits.shape}, Loss: {loss}\")\n",
    "else:\n",
    "    logits = model(idx, image_embeds)  # Call without targets\n",
    "    print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# Test generation\n",
    "generated = model.generate(idx, image_embeds, max_new_tokens=20)\n",
    "print(f\"Generated sequence shape: {generated.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3428c35-9fd4-4773-b28e-987bfe26dfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionLanguageModel(nn.Module):\n",
    "    def __init__(self, n_embd, image_embed_dim, vocab_size, n_layer, img_size, patch_size, num_heads, num_blks, emb_dropout, blk_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        # Set num_hiddens equal to image_embed_dim\n",
    "        num_hiddens = image_embed_dim\n",
    "\n",
    "        # Assert that num_hiddens is divisible by num_heads\n",
    "        assert num_hiddens % num_heads == 0, \"num_hiddens must be divisible by num_heads\"\n",
    "\n",
    "        # Initialize the vision encoder (ViT)\n",
    "        self.vision_encoder = ViT(img_size, patch_size, num_hiddens, num_heads, num_blks, emb_dropout, blk_dropout)\n",
    "\n",
    "        # Initialize the language model decoder (DecoderLanguageModel)\n",
    "        self.decoder = DecoderLanguageModel(n_embd, image_embed_dim, vocab_size, num_heads, n_layer, use_images=True)\n",
    "\n",
    "    def forward(self, img_array, idx, targets=None):\n",
    "        # Get the image embeddings from the vision encoder\n",
    "        image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "        # Check if the image embeddings are valid\n",
    "        if image_embeds.nelement() == 0 or image_embeds.shape[1] == 0:\n",
    "            raise ValueError(\"Something is wrong with the ViT model. It's returning an empty tensor or the embedding dimension is empty.\")\n",
    "\n",
    "        if targets is not None:\n",
    "            # If targets are provided, compute the logits and loss\n",
    "            logits, loss = self.decoder(idx, image_embeds, targets)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            # If targets are not provided, compute only the logits\n",
    "            logits = self.decoder(idx, image_embeds)\n",
    "            return logits\n",
    "\n",
    "    def generate(self, img_array, idx, max_new_tokens):\n",
    "        # Get the image embeddings from the vision encoder\n",
    "        image_embeds = self.vision_encoder(img_array)\n",
    "\n",
    "        # Check if the image embeddings are valid\n",
    "        if image_embeds.nelement() == 0 or image_embeds.shape[1] == 0:\n",
    "            raise ValueError(\"Something is wrong with the ViT model. It's returning an empty tensor or the embedding dimension is empty.\")\n",
    "\n",
    "        # Generate new tokens using the language model decoder\n",
    "        generated_tokens = self.decoder.generate(idx, image_embeds, max_new_tokens)\n",
    "        return generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e921d800-2bcd-499a-8ddb-d670c10532b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embed_dim = num_hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cf66c22-4a21-431c-81e4-78f1bc058821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from initialization forward pass: tensor([[[-2.5432e-01,  4.0034e-01,  1.8382e-01,  ..., -5.0611e-01,\n",
      "          -2.5905e-01,  1.4187e-01],\n",
      "         [-7.5220e-04,  3.6543e-01,  1.2834e-01,  ..., -8.7240e-01,\n",
      "           3.2104e-02,  6.7971e-01],\n",
      "         [ 1.4963e-02,  5.2306e-01,  3.5276e-01,  ..., -9.1722e-02,\n",
      "          -6.0260e-01,  9.9517e-02],\n",
      "         ...,\n",
      "         [-3.1897e-01,  3.9149e-01, -4.6407e-01,  ..., -5.4396e-01,\n",
      "           2.8357e-01,  2.0901e-01],\n",
      "         [ 4.1438e-01,  8.1610e-01, -2.7854e-01,  ..., -3.8732e-02,\n",
      "          -5.9798e-01, -1.7552e-01],\n",
      "         [ 5.2191e-01,  3.6769e-01,  3.7803e-01,  ..., -1.6366e-01,\n",
      "           7.1262e-02,  1.0142e+00]]], device='cuda:0',\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_layer, block_size =  8, 32\n",
    "\n",
    "# Initialize the model\n",
    "model = VisionLanguageModel(n_embd, image_embed_dim, vocab_size,  n_layer, img_size, patch_size, n_head, num_blks, dropout, dropout)\n",
    "model.to(device)\n",
    "\n",
    "# Create dummy data with correct dimensions\n",
    "dummy_img = torch.randn(1, 3, img_size, img_size).to(device)  # Correct shape for image input\n",
    "dummy_idx = torch.randint(0, vocab_size, (1, block_size)).to(device)  # Correct shape for text input\n",
    "\n",
    "# Forward pass to initialize all parameters\n",
    "try:\n",
    "    output = model(dummy_img, dummy_idx)  # Output for debugging\n",
    "    print(\"Output from initialization forward pass:\", output)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime Error during forward pass: {str(e)}\")\n",
    "    print(\"Check layer configurations and input shapes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c2f65e5-3f8c-4d30-844c-e5f5d63fa6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def base64_to_tensor(base64_str, img_size=96):\n",
    "    image = Image.open(io.BytesIO(base64.b64decode(base64_str)))\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)  # Add batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d22a5d68-6097-4194-9ff8-80186e0368e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting the data loader from makemore for multimodal data\n",
    "def get_batch(df, batch_size, split='train', img_size=96, val_batch_size=8):\n",
    "    # Split data into training and validation sets\n",
    "    n = int(0.9 * len(df))  # first 90% will be train, rest val\n",
    "    df_train = df.iloc[:n]\n",
    "    df_val = df.iloc[n:]\n",
    "    data = df_train if split == 'train' else df_val\n",
    "    batch_size = batch_size if split == 'train' else val_batch_size\n",
    "    replace = False if split == 'train' else True\n",
    "    batch = data.sample(n=batch_size, replace=replace)\n",
    "\n",
    "    images = torch.cat([base64_to_tensor(img, img_size) for img in batch['b64string_images']], dim=0).to(device)\n",
    "    text_indices = [torch.tensor(encode(desc), dtype=torch.long) for desc in batch['caption']]\n",
    "    max_length = max(len(t) for t in text_indices)\n",
    "\n",
    "    padded_text = torch.full((batch_size, max_length), fill_value=stoi[''], dtype=torch.long).to(device)\n",
    "    for i, text in enumerate(text_indices):\n",
    "        padded_text[i, :len(text)] = text\n",
    "\n",
    "    targets = torch.cat([padded_text[:, 1:], torch.full((batch_size, 1), fill_value=stoi[''], dtype=torch.long, device=device)], dim=1)\n",
    "\n",
    "    # Truncate or pad targets to match the length of padded_text\n",
    "    if targets.size(1) > padded_text.size(1):\n",
    "        targets = targets[:, :padded_text.size(1)]\n",
    "    elif targets.size(1) < padded_text.size(1):\n",
    "        targets = torch.cat([targets, torch.full((batch_size, padded_text.size(1) - targets.size(1)), fill_value=stoi[''], dtype=torch.long, device=device)], dim=1)\n",
    "\n",
    "    return images, padded_text, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56eac4b5-ecad-42ab-98f3-86a0d1ee584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting the training loop from makemore for multimodal data\n",
    "def train_model(model, df, epochs, vocab_size, img_size=96):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for _ in range(max_iters):\n",
    "            images, idx, targets = get_batch(df, batch_size, 'train', img_size)\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(images, idx, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if _ % eval_interval == 0:\n",
    "                print(f\"Loss at iteration {_}: {loss.item()}\")\n",
    "        val_loss = estimate_loss(model, df, 'val', img_size, val_batch_size=8)\n",
    "        print(f\"Validation Loss after epoch {epoch}: {val_loss}\")\n",
    "\n",
    "def estimate_loss(model, df, split, img_size=96, val_batch_size=8):\n",
    "    losses = []\n",
    "    model.eval()\n",
    "    for _ in range(eval_iters):\n",
    "        images, idx, targets = get_batch(df, batch_size, split, img_size, val_batch_size=val_batch_size)\n",
    "        _, loss = model(images, idx, targets)\n",
    "        losses.append(loss.item())\n",
    "    return sum(losses) / len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9c14f377-8c81-497f-824d-4ccfb3b5039d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./inputs.csv\")\n",
    "#Expanding dataframe so that there's enough data to test. This is just duplicating data. A real dataset would have more rows\n",
    "df = pd.concat([df] * 30)[['b64string_images', 'caption']]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "11779556-ac19-449a-8adf-57e330e6a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ffcbb0dd-6209-43c7-9a45-ed60f21eb83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 100\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "epochs=1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 40\n",
    "num_blks= 3\n",
    "head_size = 16\n",
    "n_embd = 128\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.1\n",
    "img_size=96\n",
    "patch_size =16\n",
    "image_embed_dim = 512\n",
    "emb_dropout = blk_dropout =0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "befee8d4-095a-493b-85ff-5e988e08e74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 4.472084045410156\n",
      "Loss at iteration 10: 0.490374356508255\n",
      "Loss at iteration 20: 0.10000506043434143\n",
      "Loss at iteration 30: 0.06568602472543716\n",
      "Loss at iteration 40: 0.036975108087062836\n",
      "Loss at iteration 50: 0.03127158060669899\n",
      "Loss at iteration 60: 0.023338178172707558\n",
      "Loss at iteration 70: 0.024506663903594017\n",
      "Loss at iteration 80: 0.02274126186966896\n",
      "Loss at iteration 90: 0.02181425876915455\n",
      "Validation Loss after epoch 0: 0.022688378463499248\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = VisionLanguageModel(n_embd, image_embed_dim, vocab_size, n_layer, img_size, patch_size, n_head, num_blks, emb_dropout, blk_dropout)\n",
    "model.to(device)\n",
    "\n",
    "# Dummy data to initialize lazy modules\n",
    "dummy_img = torch.randn(1, 3, img_size, img_size).to(device)\n",
    "dummy_idx = torch.randint(0, vocab_size, (1, block_size)).to(device)\n",
    "model(dummy_img, dummy_idx)  # Forward pass to initialize all parameters\n",
    "\n",
    "# Train the model\n",
    "train_model(model, df, epochs, vocab_size, img_size)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0622ce2a-0414-4977-9a2b-adf9abfbc275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training captions:\n",
      "0    a red sports car with big wheels\n",
      "1        an antique iphone on a table\n",
      "2             spaceship orbiting Mars\n",
      "Name: caption, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('inputs.csv')\n",
    "print(\"Training captions:\")\n",
    "print(df['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f20ca69-59be-4b5f-800e-5fbec0794d95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
